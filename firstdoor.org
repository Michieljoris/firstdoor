* good to know
** emacs setup
Use nodejs-minor mode. Do a C x,v to compile a node file and see the
output in another window
** Use filemonitor to execute js, or bash processes on file changes
Need to install filemonitor module with npm install filemonitor

** responsive images
Resize images to proper size then include with height=auto and
max-width=100%

** mind the url with hashbang routing urls:
   href: 'index.html#!/home'
   it's  #!/route following the main page (index.html in this case)

   
** resizing images:
Install imagemagick then
mogrify -resize x450 *.jpg
to resize all images in dir
and:
mogrify -quality 80 *.jpg
to compress
** recaptcha 
Domain Name: 	michieljoris.com

reCAPTCHA will only work on this domain and subdomains. If you have more than one domain (or a staging server), you can create a new set of keys.
Public Key: 	6LfL6OASAAAAAM6YHDJmCJ-51zXY1TwCL7pL7vW5

Use this in the JavaScript code that is served to your users
Private Key: 	6LfL6OASAAAAAHklMnnQdS4AmZvXuOp1ihgPY7V9

Use this when communicating between your server and our server. Be sure to keep it a secret.
* todo
  
** later 
*** search button!!!
*** chat
*** blog   
*** bootstrap tooltips for social buttons
*** learning style quiz
*** prefetch images
   possibly also the html?  image optimization using jpegtran?

    
** now    
   
*** leftside panel/door
**** The learning style quiz is tucked away under resources and then under learning,
 so it would be good to have this in the green door of links separately

 

*** clean up css for narrow screen,    
fix up ipad issues
 menu won't go away, won't scroll to id tag. 
 
links on resources page.. 
*** SEO
***** add sitemap
    http://www.sitemaps.org/protocol.html
   
***** make the site crawlable
Which means serve static pages generated by a headless browser. ok
then..
https://developers.google.com/webmasters/ajax-crawling/docs/specification
https://developers.google.com/webmasters/ajax-crawling/docs/html-snapshot
From wikipedia:
The leading search engines, such as Google, Bing and Yahoo!, use
crawlers to find pages for their algorithmic search results. Pages
that are linked from other search engine indexed pages do not need to
be submitted because they are found automatically. Some search
engines, notably Yahoo!, operate a paid submission service that
guarantee crawling for either a set fee or cost per click.[30] Such
programs usually guarantee inclusion in the database, but do not
guarantee specific ranking within the search results.[31] Two major
directories, the Yahoo Directory and the Open Directory Project both
require manual submission and human editorial review.[32] Google
offers Google Webmaster Tools, for which an XML Sitemap feed can be
created and submitted for free to ensure that all pages are found,
especially pages that are not discoverable by automatically following
links.[33]
http://www.yearofmoo.com/2012/11/angularjs-and-seo.html
https://github.com/deanmao/node-chimera
https://github.com/steeve/angular-seo
http://www.marketingpilgrim.com/7-minute-seo-guide
http://static.googleusercontent.com/external_content/untrusted_dlcp/www.google.com/en//webmasters/docs/search-engine-optimization-starter-guide.pdf

*** breadcrumbs http://static.googleusercontent.com/external_content/untrusted_dlcp/www.google.com/en//webmasters/docs/search-engine-optimization-starter-guide.pdf

*** compress assessment image
*** get rid of dataclone error
*** large hand book no problem
*** (Insert jpg image of stuctured program in here) 
in pd/trainingplans
*** course guid links
    
    
* Contact
Andrea mobile is 0437 870 932 or home p: 3491 3573.


* Questions:
  Resources can be organized better and displayed better.
  I'm trying to make it so that you can add resources yourself and edit
  them.  
 


