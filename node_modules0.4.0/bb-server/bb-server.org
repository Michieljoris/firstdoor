** bb-server
*** considerations 
    load diff of (js files) instead of whole file?
    share js files between server and client?
    memory cache
disk cache
separate cache out?

crawlbots
modified since
sending out cache headers
cache busting (stripping stamp)

recast files
optimize images

authentication
logs
cors
security
sitemap

Hardcoded requests for static resources should be stamped with the
resources last modified data (in htm-builder). When the resource
changes, the site will have to be rebuilt, but it will garantue that
the requests for files that changeed and only them will get through to
bb-server

Dynamic requests for static resources from client can also be
timestamped, they will be sent out with max-age > 0, this only makes
sens when the stamp is tied to another file that gives them a fixed
stamp to attach, for example in index.html;

Don't have a favicon.ico in the rootfolder, but add it to the server
options so you can easily control cache header expire

*** other servers:   
 https://github.com/mjijackson/mach  
 Simplicity: straightforward mapping of HTTP requests to JavaScript function calls
Streaming: request and response bodies can be streamed
Composability: middleware composes easily using promises
Robust: Promises propagate errors up the call stack, simplifying error handling
*** crawlers
**** send real 404's
     http://prerender.io/server
     
**** make the site crawlable
Which means serve static pages generated by a headless browser. ok
then..
https://developers.google.com/webmasters/ajax-crawling/docs/specification
https://developers.google.com/webmasters/ajax-crawling/docs/html-snapshot
From wikipedia:
The leading search engines, such as Google, Bing and Yahoo!, use
crawlers to find pages for their algorithmic search results. Pages
that are linked from other search engine indexed pages do not need to
be submitted because they are found automatically. Some search
engines, notably Yahoo!, operate a paid submission service that
guarantee crawling for either a set fee or cost per click.[30] Such
programs usually guarantee inclusion in the database, but do not
guarantee specific ranking within the search results.[31] Two major
directories, the Yahoo Directory and the Open Directory Project both
require manual submission and human editorial review.[32] Google
offers Google Webmaster Tools, for which an XML Sitemap feed can be
created and submitted for free to ensure that all pages are found,
especially pages that are not discoverable by automatically following
links.[33]
http://www.yearofmoo.com/2012/11/angularjs-and-seo.html
https://github.com/deanmao/node-chimera
https://github.com/steeve/angular-seo
http://www.marketingpilgrim.com/7-minute-seo-guide
http://static.googleusercontent.com/external_content/untrusted_dlcp/www.google.com/en//webmasters/docs/search-engine-optimization-starter-guide.pdf
**** Recognizing bots: 
***** Client asks or escaped fragment:
 https://developers.google.com/webmasters/ajax-crawling/docs/specification
 https://developers.google.com/webmasters/ajax-crawling/docs/getting-started
***** regex the user string:
   robot|spider|crawler|curl|slurp or:
   bot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo!
   slurp|mediapartners-google
   or:
   http://stackoverflow.com/questions/544450/detecting-honest-web-crawlers
***** or consult a database:
   weekly updated cached parser 
    https://github.com/GUI/uas-parser
    https://npmjs.org/package/uas-parser
    
    or use the standard one, but no bot data:
    https://npmjs.org/package/useragent
    
**** serve static version:    
     https://github.com/moviepilot/seoserver
     Installs phantomjs as a node module, uses exec
     https://github.com/Obvious/phantomjs
     Phantom api
https://github.com/ariya/phantomjs/wiki/API-Reference-WebPage#wiki-webpage-plainText
More info on phantom:
http://thedigitalself.com/blog/seo-and-javascript-with-phantomjs-server-side-rendering
http://www.sitepoint.com/headless-webkit-and-phantomjs/
**** More    
   identifying googlebot by ip:
   by reverse dns lookup of ip, has to be in googlebot.com domain
   then forward dns lookup, which should get you your ip back again.
   https://support.google.com/webmasters/answer/80553?hl=en
   How to identify search engine spiders and webbots
   http://www.jafsoft.com/searchengines/spider_hunting.html
    http://stackoverflow.com/questions/544450/detecting-honest-web-crawlers
**** implementation   
    https://github.com/icodeforlove/node-express-renderer
    https://github.com/steeve/angular-seo
    http://www.yearofmoo.com/2012/11/angularjs-and-seo.html
    https://github.com/markselby/node-angular-seo
    https://github.com/bfirsh/otter
    http://backbonetutorials.com/seo-for-single-page-apps/
    https://github.com/apiengine/seoserver
    
    
*** images
    http://tinypng.org/
    http://www.smushit.com/ysmush.it/
    https://kraken.io/web-interface
   drop images onto the tool and they get compressed 
   http://trimage.org/ 
   
   convert from jpg to png:
   mogrify -format jpg *.png  
   shell script:
   for img in *.png; do
    filename=${img%.*}
    convert "$filename.png" "$filename.jpg"
done
http://superuser.com/questions/71028/batch-converting-png-to-jpg-in-linux
   resizing 
Install imagemagick then
mogrify -resize x450 *.jpg
to resize all images in dir
and:
mogrify -quality 80 *.jpg
to compress
   jpegtran is in libjpeg-turbo-progs 
   pngcrush
  optipng 
  pngquant
  pngout
   Proxy them 
 https://github.com/discore/iproxy
Resize them:
  https://npmjs.org/package/grunt-image-resize
  https://npmjs.org/package/image-shrink
 Optimize:
 https://npmjs.org/package/imageoptmizer-brunch
 https://npmjs.org/package/imagemin
 https://npmjs.org/package/grunt-pngmin
 
*** security!!! 
  http://www.adambarth.com/papers/2008/barth-jackson-mitchell-b.pdf
  http://shiflett.org/articles/session-hijacking
  https://developer.mozilla.org/en-US/docs/Mozilla/Persona/Security_Considerations?redirectlocale=en-US&redirectslug=Persona%2FSecurity_Considerations
  
****  Implement CSRF protection

In a CSRF (Cross-Site Request Forgery) login attack, an attacker uses
a cross-site request forgery to log the user into a web site using the
attacker's credentials.

For example: a user visits a malicious web site containing a form
element. The form's action attribute is set to an HTTP POST request to
http://www.google.com/login, supplying the attacker's username and
password. When the user submits the form, the request is sent to
Google, the login succeeds and the Google server sets a cookie in the
user's browser. Now the user's unknowingly logged into the attacker's
Google account.

The attack can be used to gather sensitive information about the
user. For example, Google's Web History feature logs all the user's
Google search terms. If a user is logged into the attacker's Google
account and the attacker has Web History enabled, then the user is
giving the attacker all this information.

CSRF login attacks, and potential defenses against them, are
documented more fully in Robust Defenses for Cross-Site Request
Forgery (PDF). They're not specific to Persona: most login mechanisms
are potentially vulnerable to them.

There are a variety of techniques which can be used to protect a site
from CSRF login attacks, which are documented more fully in the study
above.

One approach is to create a secret identifier in the server, shared
with the browser, and require the browser to supply it when making
login requests. For example:

As soon as the user lands on your site, before they try to log in,
create a session for them on the server. Store the session ID in a
browser cookie.  On the server, generate a random string of at least
10 alphanumeric characters. A randomly generated UUID is a good
option. This is the CSRF token. Store it in the session.  Deliver the
CSRF token to the browser by either embedding it in JavaScript or HTML
as a hidden form variable.  Ensure that the AJAX submission or form
POST includes the CSRF token.  On the server side, before accepting an
assertion, check that the submitted CSRF token matches the
session-stored CSRF token.
**** angular security 
http://docs.angularjs.org/api/ng.$http
   
**** use secure cookies:
https://github.com/jed/cookies
https://github.com/jed/keygrip
http://mahoney.eu/2012/05/23/couchdb-cookie-authentication-nodejs-nano/#.UbAdzqBCAWM
**** csrf
    look at connect middleware for implementation 
*** enable cors
    https://github.com/agrueneberg/Corser
    https://github.com/troygoode/node-cors
    send a bunch of headers and respond to options method when
    enabled. Use couchdb setup as an example for settings
*** option for spa: 
always send index.html when requesting non-file
when serving spa and you don't want to use #! you always serve
index.html and then let the app sort out the routing.
http://docs.angularjs.org/guide/dev_guide.services.$location
*** sign in with
    google, facebook, linkedin, github, persona, twitter, basic
    to start of with, incorporate persona into server
    
*** TODO
**** Keep logs!!!   
    
    //TODO merge request logs with the console log
    http://logio.org/
**** clean up dichotomy of log and silent    
   Should have status out and error out and server out  
    http://www.senchalabs.org/connect/logger.html
**** use winston and its transport ipv file, also has logrotation
**** logrotation:
    You can use logrotate which is included in most Linux distributions and is used for rotating system log files, as well as used by other software like Apache.

Add a file to /etc/logrotate.d/

/path/to/server.log {
  daily         # how often to rotate
  rotate 10     # max num of log files to keep
  missingok     # don't panic if the log file doesn't exist
  notifempty    # ignore empty files
  compress      # compress rotated log file with gzip
  sharedscripts # no idea what it does, but it's in all examples
  copytruncate  # needed for forever to work properly
  dateext       # adds date to filename 
  dateformat %Y-%m-%d.
}
http://www.thegeekstuff.com/2010/07/logrotate-examples/
    
**** server reporting
***** access server logs in browser?    
    https://github.com/ethanl/connect-browser-logger
    add a get handler for example /__logs and serve page with stats
    possibly only when authorized using persona for example
***** -report to console:
https://github.com/ethanl/connect-browser-logger
***** -airbrake like, so post info somewhere
- use loggly , or newrelic
**** serve fancy dir
http://www.senchalabs.org/connect/directory.html
with icons, json as json, html as html, js as js, possibly with
highlighting etc, show hidden files?

**** test and clean up forwarder!!    
    I put it in a module, but is not tested yet

**** send script that listens to sockets and refreshes browser
    ala livereload perhaps, skewer??
    

*** ??
**** cache in couchdb?
**** threshold for gzipping?
**** use nodemon?   
    nodemon will watch the files in the directory that nodemon was
    started, and if they change, it will automatically restart your
    node application.
  https://github.com/rem
   
